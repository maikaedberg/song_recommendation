{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Collaborative-Filtering Recommendation Sytem from Scratch\n",
    "Music recommendation system can be broadly categorised into three categories: content-based, collaborative filtering, and a hybrid of the two. In a content-based recommendation system, a user profile is built based on their likes and dislikes. The algorithm then analyses the items and their descriptions to determine the best possible choice for the user$^1$. On the other hand, collaborative filtering has two approaches: user-based and item-based. A user-based collaborative filtering recommendation system “relies on the fact each person belongs in a group of similarly behaving individuals” $^2$. By taking into account the actions of the group, the individuals behaviour can be predicted. In an item-based approach, the similarities between the products are computed, then the system will infer likes and dislikes based on whether or not the products are similar to what they have already purchased. In a hybrid approach, most combine collaborative filtering with some from of content-based filtering with a certain weight $^3$.\n",
    "\n",
    "We will be testing our implementation on [The Echo Nest Taste Profile Subset](http://millionsongdataset.com/tasteprofile/), in the challenge introduced at the [Million Song Dataset Challenge](https://www.kaggle.com/c/msdchallenge/data). We will be working on the smaller subset, where we have information on $10000$ users, across $49029$ songs, provided by $131039$ triplets (under data/valid_triplets_hidden.txt and data/valid_triplets_visible.txt. For computational reasons, we will not be working on the larger susbset, containing $100000$ users, $157251$ songs, and $1319894$ triplets (under data/test_triplets_hidden.txt and data/test_triplets_visible.txt). However, with a more powerful computer, applying the same algorithm is not be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation ###\n",
    "From this point on, we will be using the algorithm introdcued in [this research paper](https://dl.acm.org/doi/10.1145/963770.963776). The notation will be consistent of that in the paper. Namely, $n$ will denote the number of distinct users, $m$ the number of distinct items, $R$ the $n \\times m$ *user-item matrix*, where the $R_{i, j}$ corresponds to how many times user $i$ played the song $j$. Because noramlization of each row improved the result in all cases in the research paper, all rows are normalized, dividing each item by the Euclidean norm. The matrix $M$ represents the *similarity matrix*, where $M_{i,j}$ is the similairity measure between song $i$ and $j$ if $i \\neq j$ and $0$ otherwise. Finally the vector $U$ generally denotes the users' basket, i.e. the items that they bought. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_triplet = 'data/evaldata/valid_triplets_visible.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_users_songs(file_triple):\n",
    "    \"\"\" given file_triplet of the form: user_id \\t song_id \\t play_times \\n\n",
    "        returns two dictionaries, associates users and songs to their index\n",
    "        returns two lists,        associates indexes to the users and songs\"\"\"\n",
    "    \n",
    "    unique_users, unique_songs  = dict(), dict()\n",
    "    users, songs = [], []\n",
    "    \n",
    "    with open(file_triplet, \"r\") as f:\n",
    "        i, j = 0, 0\n",
    "        for l in f:\n",
    "            \n",
    "            user_id, song_id, _ = l.split('\\t')\n",
    "            \n",
    "            if user_id not in unique_users:\n",
    "                unique_users[user_id] = i\n",
    "                users.append(user_id)\n",
    "                i += 1\n",
    "                \n",
    "            if song_id not in unique_songs:\n",
    "                unique_songs[song_id] = j\n",
    "                songs.append(song_id)\n",
    "                j += 1\n",
    "                  \n",
    "    return unique_users, unique_songs, users, songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_info(file_triplet, user_dict, song_dict):\n",
    "    \"\"\"\n",
    "    given the file_triplet and the mapping between the user_id and the indexes, \n",
    "    returns data, row_ind, col_ind, n, m\n",
    "    where \n",
    "    \"\"\"\n",
    "    # user_id \\t song \\t no. of times\n",
    "    n, m = len(user_dict), len(song_dict)\n",
    "    \n",
    "    # build this sparse matrix\n",
    "    data, row_ind, col_ind = [], [], []\n",
    "    with open(file_triplet, \"r\") as f:\n",
    "        prev_user = ''\n",
    "        curr_data = []\n",
    "\n",
    "        for l in f:\n",
    "            user_id, song_id, freq = l.split('\\t')\n",
    "            row_ind.append(user_dict[user_id])\n",
    "            col_ind.append(song_dict[song_id])\n",
    "            curr_data.append(int(freq.strip('\\n')))\n",
    "            \n",
    "            # normalization\n",
    "            if prev_user != user_id:\n",
    "                data += [i / np.linalg.norm(curr_data) for i in curr_data]\n",
    "                curr_data = []\n",
    "                \n",
    "            prev_user = user_id\n",
    "            \n",
    "    data += [i / sum(curr_data) for i in curr_data]\n",
    "    \n",
    "    return data, row_ind, col_ind, n, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict, song_dict, users, songs = load_users_songs(file_triplet)\n",
    "data, row_ind, col_ind, n, m = build_info(file_triplet, user_dict, song_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_R_col(data, row_ind, col_ind, n, m):\n",
    "    return csc_matrix((data, (row_ind, col_ind)), shape=(n,m))\n",
    "\n",
    "def build_R_row(data, row_ind, col_ind, n, m):\n",
    "    return csr_matrix((data, (row_ind, col_ind)), shape=(n,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_row = build_R_row(data, row_ind, col_ind, n, m)\n",
    "R_col = build_R_row(data, row_ind, col_ind, n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_M_cos(R_col):\n",
    "    M = cosine_similarity(R_col.transpose(), dense_output = False)\n",
    "    for i in range(M.shape[0]):\n",
    "        M[i,i] = 0\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_cos = build_M_cos(R_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built the similarity matrix $M$ using the cosine similarity measure, we will be building the similarity matrix $M$ using the conditional probability measure using the formula introduced in section 4.1.1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_user_dict(triple_file, song_dict, user_dict):\n",
    "    # res[song_index] = list of user_index that bought the song\n",
    "    res = dict()\n",
    "    \n",
    "    with open(file_triplet, \"r\") as f:\n",
    "        for l in f:\n",
    "            user_id, song_id, _ = l.split('\\t')\n",
    "            if song_dict[song_id] in res:\n",
    "                res[song_dict[song_id]].append(user_dict[user_id])\n",
    "            else:\n",
    "                res[song_dict[song_id]] = [user_dict[user_id]]\n",
    "                \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_user = song_user_dict(file_triplet, song_dict, user_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(A, B):\n",
    "    # https://stackoverflow.com/questions/29201260/a-fast-way-to-find-the-number-of-elements-in-list-intersection-python\n",
    "    fst, snd = (A, B) if len(A) < len(B) else (B, A)\n",
    "    return len(set(fst).intersection(snd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes a lot of time computationally, have already run it so we can load it with pickle\n",
    "def build_M_prob(song_user):\n",
    "    data, row_ind, col_ind = [], [], []\n",
    "    m = len(song_user)\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            if i != j:\n",
    "                freq_ij = intersection(song_user[i], song_user[j])\n",
    "                if freq_ij != 0:\n",
    "                    data.append(freq_ij)\n",
    "                    row_ind.append(i)\n",
    "                    col_ind.append(j)\n",
    "                    \n",
    "    return csc_matrix((data, (row_ind, col_ind)), shape=(m,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M_prob = build_M_prob(song_user)\n",
    "# file_to_write = open(\"M_prob.pickle\", \"wb\")\n",
    "# pickle.dump(M_prob, file_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_prob = pickle.load(open(\"M_prob.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the model, using the psuedo-code on how to apply the similarity matirx $M$ we have just created (Algorithm 4.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id, user_dict, songs, R_row, M, k = 10):\n",
    "    \n",
    "    U = R_row.getrow(user_dict[user_id]).transpose()\n",
    "    x = M.dot(U).getcol(0).transpose().todense().A[0]\n",
    "    \n",
    "    for j in range(m):\n",
    "        if j in U.indices:\n",
    "            x[j] = 0\n",
    "    \n",
    "    return set(songs[j] for j in x.argsort()[:-k-1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SOIUMKH12A6D4F91A4',\n",
       " 'SOPTXHV12AB01889F1',\n",
       " 'SOQOAGV12A8C132F7D',\n",
       " 'SOSATQZ12AB017DF2F',\n",
       " 'SOUCKTI12AB017DF15',\n",
       " 'SOUGPVV12AB0186397',\n",
       " 'SOUOULX12A8C13A4F8',\n",
       " 'SOUTTSQ12A6D4F43FA',\n",
       " 'SOWOSHB12AF72A237C',\n",
       " 'SOXQMFT12A58A7AA88'}"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of 10 recommendations\n",
    "recommend(\"0007140a3796e901f3190f12e9de6d7548d4ac4a\", user_dict, songs, R_row, M_cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the accuracy of the system. We built recommendations for each user, associating each user_id with a set of song_ids that were recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendations_cos = dict()\n",
    "# for u in users:\n",
    "#     recommendations_cos[u] = recommend(u, user_dict, songs, R_row, M_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommendations_prob = dict()\n",
    "# for u in users:\n",
    "#    recommendations_prob[u] = recommend(u, user_dict, songs, R_row, M_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_to_write = open(\"recommendations_cos.pickle\", \"wb\")\n",
    "# pickle.dump(recommendations_cos, file_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_to_write = open(\"recommendations_prob.pickle\", \"wb\")\n",
    "# pickle.dump(recommendations_prob, file_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_cos = pickle.load(open(\"recommendations_cos.pickle\", \"rb\"))\n",
    "recommendations_prob = pickle.load(open(\"recommendations_prob.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_set(hidden_triplet, users):\n",
    "    res = {u : set() for u in users}\n",
    "    with open(hidden_triplet, \"r\") as f:\n",
    "        for l in f:\n",
    "            user_id, song_id, _ = l.split('\\t')\n",
    "            res[user_id].add(song_id)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_result = valid_set(\"data/evaldata/valid_triplets_hidden.txt\", users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the accuracy of our model, we use the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index) where \n",
    "$$\n",
    "    J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n",
    "$$\n",
    "It is used to compute the similarity of two sample sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Cosine-Based Similarity-Measure:             0.012384627325967344\n",
      "Performance of Conditional Probability Similarity-Measure:  0.0373200247213094\n",
      "Performance of Random Recommendation:                       0.0373200247213094\n",
      "Cosine performs                  81.75x better than random.\n",
      "Conditional probability performs 246.36x better than random.\n"
     ]
    }
   ],
   "source": [
    "J_ind_cos, J_ind_prob, J_ind_random = [], [], []\n",
    "\n",
    "for user, res in true_result.items():\n",
    "    recs_cos = recommendations_cos[user]\n",
    "    recs_prob = recommendations_prob[user]\n",
    "    recs_random = set(random.sample(songs_cpy, 10))\n",
    "    \n",
    "    J_ind_cos.append( len(recs_cos.intersection(res)) / len(recs_cos.union(res)) )\n",
    "    J_ind_prob.append( len(recs_prob.intersection(res)) / len(recs_prob.union(res)) )\n",
    "    J_ind_random.append( len(recs_random.intersection(res)) / len(recs_random.union(res)) )\n",
    "    \n",
    "performance_cos =  np.array(J_ind_cos).mean()\n",
    "performance_prob = np.array(J_ind_prob).mean()\n",
    "performance_random = np.array(J_ind_random).mean()\n",
    "    \n",
    "print(\"Performance of Cosine-Based Similarity-Measure:            \", np.array(Jaccard_indices_cos).mean())\n",
    "print(\"Performance of Conditional Probability Similarity-Measure: \", np.array(Jaccard_indices_prob).mean())\n",
    "print(\"Performance of Random Recommendation:                      \", np.array(Jaccard_indices_prob).mean())\n",
    "print(\"Cosine performs                  {}x better than random.\".format(round(performance_cos / performance_random, 2)))\n",
    "print(\"Conditional probability performs {}x better than random.\".format(round(performance_prob / performance_random, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
